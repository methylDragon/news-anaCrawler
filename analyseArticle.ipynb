{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analyseArticle\n",
    "***Author: methylDragon (methylDragon.com)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description: (Sentence) tokenises article from inputted URL, and conducts NLP analysis to draw out important parameters.\n",
    "{\"title\", \"url\", \"authors\", \"date\", \"summary\", \"polarity\", \"subjectivity\", \"keywords\", \"images\", \"videos\", \"text\"}\n",
    "\n",
    "To use, insert %run 'analyseArticle.ipynb' after your import statement.\n",
    "\n",
    "then use\n",
    "\n",
    "result = analyseArticle(URL)\n",
    "result[\"PARAMETER\"]\n",
    "\n",
    "To get your wanted parameter!\n",
    "\n",
    "eg. result[\"title\"] #outputs title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NLP\n",
    "from pattern.web import Twitter\n",
    "from textblob import TextBlob\n",
    "import nltk.data\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "#NLTK RESOURCE DOWNLOADING\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "#PARSER\n",
    "from newspaper import Article\n",
    "import newspaper\n",
    "\n",
    "#set tokenizer model\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analyseArticle(url):\n",
    "#list, download, and parse article\n",
    "    a = Article(str(url))\n",
    "    try:\n",
    "        a.download()\n",
    "        a.parse()\n",
    "        a.nlp()\n",
    "    except:\n",
    "        return \"FETCH_ERROR\"\n",
    "    \n",
    "    #Overall sentiment trackers\n",
    "    count = 0\n",
    "    overallScore = [0.0,0.0]\n",
    "\n",
    "    sentences = []\n",
    "    \n",
    "    for index, token in enumerate(tokenizer.tokenize(a.text)):\n",
    "        sentences.append(TextBlob(tokenizer.tokenize(a.text)[index]))\n",
    "    \n",
    "    #Split article into sentences\n",
    "    for index, token in enumerate(tokenizer.tokenize(a.text)):\n",
    "        \n",
    "        analysis = sentences[index]\n",
    "        # analysis.correct() #Correct mispelt words !!! IF YOU ACTIVATE THIS IT'LL BE SLOW \n",
    "        \n",
    "        #and for each sentence, analyze sentiment\n",
    "        #Prep overall analysis tracker, IGNORE if parameters are [0.0, 0.0] for sentence\n",
    "        if analysis.sentiment.polarity != 0.0 and analysis.sentiment.subjectivity != 0.0:\n",
    "            count += 1\n",
    "            overallScore[0] += analysis.sentiment.polarity\n",
    "            overallScore[1] += analysis.sentiment.subjectivity\n",
    "        else:\n",
    "            try:\n",
    "                sentences[index + 1] = sentences[index] + \" \" + sentences[index + 1]\n",
    "                analysis = sentences[index + 1]\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    #Guarding against divisions by 0\n",
    "    if count == 0:\n",
    "        count = 1    \n",
    "    \n",
    "    #Store variables\n",
    "    title = a.title\n",
    "    authors = a.authors\n",
    "    date = a.publish_date\n",
    "    summary = a.summary\n",
    "    polarity = overallScore[0]/count\n",
    "    subjectivity = overallScore[1]/count\n",
    "    if polarity == 0.0 or subjectivity == 0.0:\n",
    "        return \"ZERO_SENTIMENT_ERROR\"\n",
    "    keywords = a.keywords\n",
    "    images = a.top_image\n",
    "    videos = a.movies\n",
    "    text = a.text\n",
    "    language = getattr(a,\"meta_lang\")\n",
    "\n",
    "    #parameters = [title, url, authors, date, summary, polarity, subjectivity, keywords, images, videos, text, language]\n",
    "\n",
    "    #Output parameter variables in a dictionary as function output\n",
    "    return {\"title\":title, \"url\":url, \"authors\":authors, \"date\":date, \"summary\":summary, \"polarity\":polarity, \"subjectivity\":subjectivity, \"keywords\":keywords, \"images\":images, \"videos\":videos, \"text\":text, \"language\": language}\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
